
## äºŒã€chp2

### 1. np.outer, å¤–ç§¯è®¡ç®—

```python
expected(i, j) = col_totals[i] * row_totals[j] # è®¡ç®—pmiçš„æ—¶å€™å¯ä»¥ç”¨åˆ°
```

### 2. svd

- ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡:https://www.bilibili.com/video/BV1vY4y1J7gd?spm_id_from=333.337.search-card.all.click&vd_source=953ee6e625bf100ef80ae35363880522
```python
A*v1 = l*v1: è¿™é‡ŒçŸ©é˜µåªèµ·åˆ°äº†ä¼¸ç¼©çš„ä½œç”¨ï¼Œlå°±æ˜¯ç‰¹å¾å€¼ï¼Œv1å°±æ˜¯ç‰¹å¾å‘é‡ã€‚

```
- svdä»‹ç»:https://www.bilibili.com/video/BV1Hq4y117VK?spm_id_from=333.337.search-card.all.click&vd_source=953ee6e625bf100ef80ae35363880522
```python
W(m*n) = U(æ—‹è½¬ï¼Œm*m)*s(æ‹‰ä¼¸, m*n)*Vh(æ—‹è½¬, n*n)
```

- svdç”¨æ³•ï¼Œé™ä½çº¬åº¦:https://www.cnblogs.com/pinard/p/6251584.html
```
å¯¹äºå¥‡å¼‚å€¼,å®ƒè·Ÿæˆ‘ä»¬ç‰¹å¾åˆ†è§£ä¸­çš„ç‰¹å¾å€¼ç±»ä¼¼ï¼Œåœ¨å¥‡å¼‚å€¼çŸ©é˜µä¸­ä¹Ÿæ˜¯æŒ‰ç…§ä»å¤§åˆ°å°æ’åˆ—ï¼Œè€Œä¸”å¥‡å¼‚å€¼çš„å‡å°‘ç‰¹åˆ«çš„å¿«ï¼Œåœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œå‰10%ç”šè‡³1%çš„å¥‡å¼‚å€¼çš„å’Œå°±å äº†å…¨éƒ¨çš„å¥‡å¼‚å€¼ä¹‹å’Œçš„99%ä»¥ä¸Šçš„æ¯”ä¾‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨æœ€å¤§çš„kä¸ªçš„å¥‡å¼‚å€¼å’Œå¯¹åº”çš„å·¦å³å¥‡å¼‚å‘é‡æ¥è¿‘ä¼¼æè¿°çŸ©é˜µã€‚ä¹Ÿå°±æ˜¯è¯´å…¶ä¸­kè¦æ¯”nå°å¾ˆå¤šï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªå¤§çš„çŸ©é˜µAå¯ä»¥ç”¨ä¸‰ä¸ªå°çš„çŸ©é˜µğ‘ˆğ‘šÃ—ğ‘˜,Î£ğ‘˜Ã—ğ‘˜,ğ‘‰ğ‘‡ğ‘˜Ã—ğ‘›æ¥è¡¨ç¤ºã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç°åœ¨æˆ‘ä»¬çš„çŸ©é˜µAåªéœ€è¦ç°è‰²çš„éƒ¨åˆ†çš„ä¸‰ä¸ªå°çŸ©é˜µå°±å¯ä»¥è¿‘ä¼¼æè¿°äº†ã€‚
```

## äºŒã€chp3

### åºåˆ—æ ‡æ³¨

- crf

- ç»´ç‰¹æ¯”ç®—æ³•ï¼ˆViterbiï¼‰è§£ç ç®—æ³•

- è¯­æ³•æ ‘è§£æ

- ä¾å­˜å¥æ³•åˆ†æ

- hmm:ã€æ•°ä¹‹é“ 29ã€‘"éšé©¬å°”å¯å¤«æ¨¡å‹"HMMæ˜¯ä»€ä¹ˆï¼Ÿäº†è§£å®ƒåªéœ€5åˆ†é’Ÿï¼https://www.bilibili.com/video/BV1ZB4y1y7gC?spm_id_from=333.337.search-card.all.click&vd_source=953ee6e625bf100ef80ae35363880522



## å››ã€åŸºç¡€ä»£ç å­¦ä¹ 

### 4.1 mlp
```python
import torch
from torch import nn, optim
from torch.nn import functional as F

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_class):
        super(MLP, self).__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.activate = F.relu
        self.linear2 = nn.Linear(hidden_dim, num_class)

    def forward(self, inputs):
        hidden = self.linear1(inputs)
        activation = self.activate(hidden)
        outputs = self.linear2(activation)
        # è·å¾—æ¯ä¸ªè¾“å…¥å±äºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡ï¼ˆSoftmaxï¼‰ï¼Œç„¶åå†å–å¯¹æ•°
        # å–å¯¹æ•°çš„ç›®çš„æ˜¯é¿å…è®¡ç®—Softmaxæ—¶å¯èƒ½äº§ç”Ÿçš„æ•°å€¼æº¢å‡ºé—®é¢˜
        # log_softmax + NLLLoss() å°±æ˜¯äº¤å‰ç†µçš„ç»“æœ
        # Pytorchè¯¦è§£NLLLosså’ŒCrossEntropyLoss : https://blog.51cto.com/u_15274944/2921745
        # æŸå¤±å‡½æ•°ï½œäº¤å‰ç†µæŸå¤±å‡½æ•° ï¼šhttps://zhuanlan.zhihu.com/p/35709485
        log_probs = F.log_softmax(outputs, dim=1)
        return log_probs

# å¼‚æˆ–é—®é¢˜çš„4ä¸ªè¾“å…¥
x_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
# æ¯ä¸ªè¾“å…¥å¯¹åº”çš„è¾“å‡ºç±»åˆ«
y_train = torch.tensor([0, 1, 1, 0])

# åˆ›å»ºå¤šå±‚æ„ŸçŸ¥å™¨æ¨¡å‹ï¼Œè¾“å…¥å±‚å¤§å°ä¸º2ï¼Œéšå«å±‚å¤§å°ä¸º5ï¼Œè¾“å‡ºå±‚å¤§å°ä¸º2ï¼ˆå³æœ‰ä¸¤ä¸ªç±»åˆ«ï¼‰
model = MLP(input_dim=2, hidden_dim=5, num_class=2)

criterion = nn.NLLLoss() # å½“ä½¿ç”¨log_softmaxè¾“å‡ºæ—¶ï¼Œéœ€è¦è°ƒç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆNegative Log Likelihoodï¼ŒNLLï¼‰
optimizer = optim.SGD(model.parameters(), lr=0.05) # ä½¿ç”¨æ¢¯åº¦ä¸‹é™å‚æ•°ä¼˜åŒ–æ–¹æ³•ï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸º0.05

for epoch in range(100):
    y_pred = model(x_train) # è°ƒç”¨æ¨¡å‹ï¼Œé¢„æµ‹è¾“å‡ºç»“æœ
    loss = criterion(y_pred, y_train) # é€šè¿‡å¯¹æ¯”é¢„æµ‹ç»“æœä¸æ­£ç¡®çš„ç»“æœï¼Œè®¡ç®—æŸå¤±
    optimizer.zero_grad() # åœ¨è°ƒç”¨åå‘ä¼ æ’­ç®—æ³•ä¹‹å‰ï¼Œå°†ä¼˜åŒ–å™¨çš„æ¢¯åº¦å€¼ç½®ä¸ºé›¶ï¼Œå¦åˆ™æ¯æ¬¡å¾ªç¯çš„æ¢¯åº¦å°†è¿›è¡Œç´¯åŠ 
    loss.backward() # é€šè¿‡åå‘ä¼ æ’­è®¡ç®—å‚æ•°çš„æ¢¯åº¦
    optimizer.step() # åœ¨ä¼˜åŒ–å™¨ä¸­æ›´æ–°å‚æ•°ï¼Œä¸åŒä¼˜åŒ–å™¨æ›´æ–°çš„æ–¹æ³•ä¸åŒï¼Œä½†æ˜¯è°ƒç”¨æ–¹å¼ç›¸åŒ

print("Parameters:")
for name, param in model.named_parameters():
    print (name, param.data)

y_pred = model(x_train)
print("Predicted results:", y_pred.argmax(axis=1))

```

### 4.2 embedding
```python
import torch
from torch import nn
from torch.nn import functional as F

class MLP(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):
        super(MLP, self).__init__()
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # çº¿æ€§å˜æ¢ï¼šè¯åµŒå…¥å±‚->éšå«å±‚
        self.linear1 = nn.Linear(embedding_dim, hidden_dim)
        # ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
        self.activate = F.relu
        # çº¿æ€§å˜æ¢ï¼šæ¿€æ´»å±‚->è¾“å‡ºå±‚
        self.linear2 = nn.Linear(hidden_dim, num_class)

    def forward(self, inputs):
        # B*S = 2 * 4
        embeddings = self.embedding(inputs)
        # å°†åºåˆ—ä¸­å¤šä¸ªembeddingè¿›è¡Œèšåˆï¼ˆæ­¤å¤„æ˜¯æ±‚å¹³å‡å€¼ï¼‰
        embedding = embeddings.mean(dim=1)
        hidden = self.activate(self.linear1(embedding))
        outputs = self.linear2(hidden)
        # è·å¾—æ¯ä¸ªåºåˆ—å±äºæŸä¸€ç±»åˆ«æ¦‚ç‡çš„å¯¹æ•°å€¼
        probs = F.log_softmax(outputs, dim=1)
        return probs

mlp = MLP(vocab_size=8, embedding_dim=3, hidden_dim=5, num_class=2)
# è¾“å…¥ä¸ºä¸¤ä¸ªé•¿åº¦ä¸º4çš„æ•´æ•°åºåˆ—
inputs = torch.tensor([[0, 1, 2, 1], [4, 6, 6, 7]], dtype=torch.long)
outputs = mlp(inputs)
print(outputs)
```