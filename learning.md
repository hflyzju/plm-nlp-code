
## äºŒã€chp2

### 1. np.outer, å¤–ç§¯è®¡ç®—

```python
expected(i, j) = col_totals[i] * row_totals[j] # è®¡ç®—pmiçš„æ—¶å€™å¯ä»¥ç”¨åˆ°
```

### 2. svd

- ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡:https://www.bilibili.com/video/BV1vY4y1J7gd?spm_id_from=333.337.search-card.all.click&vd_source=953ee6e625bf100ef80ae35363880522
```python
A*v1 = l*v1: è¿™é‡ŒçŸ©é˜µåªèµ·åˆ°äº†ä¼¸ç¼©çš„ä½œç”¨ï¼Œlå°±æ˜¯ç‰¹å¾å€¼ï¼Œv1å°±æ˜¯ç‰¹å¾å‘é‡ã€‚

```
- svdä»‹ç»:https://www.bilibili.com/video/BV1Hq4y117VK?spm_id_from=333.337.search-card.all.click&vd_source=953ee6e625bf100ef80ae35363880522
```python
W(m*n) = U(æ—‹è½¬ï¼Œm*m)*s(æ‹‰ä¼¸, m*n)*Vh(æ—‹è½¬, n*n)
```

- svdç”¨æ³•ï¼Œé™ä½çº¬åº¦:https://www.cnblogs.com/pinard/p/6251584.html
```
å¯¹äºå¥‡å¼‚å€¼,å®ƒè·Ÿæˆ‘ä»¬ç‰¹å¾åˆ†è§£ä¸­çš„ç‰¹å¾å€¼ç±»ä¼¼ï¼Œåœ¨å¥‡å¼‚å€¼çŸ©é˜µä¸­ä¹Ÿæ˜¯æŒ‰ç…§ä»å¤§åˆ°å°æ’åˆ—ï¼Œè€Œä¸”å¥‡å¼‚å€¼çš„å‡å°‘ç‰¹åˆ«çš„å¿«ï¼Œåœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œå‰10%ç”šè‡³1%çš„å¥‡å¼‚å€¼çš„å’Œå°±å äº†å…¨éƒ¨çš„å¥‡å¼‚å€¼ä¹‹å’Œçš„99%ä»¥ä¸Šçš„æ¯”ä¾‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨æœ€å¤§çš„kä¸ªçš„å¥‡å¼‚å€¼å’Œå¯¹åº”çš„å·¦å³å¥‡å¼‚å‘é‡æ¥è¿‘ä¼¼æè¿°çŸ©é˜µã€‚ä¹Ÿå°±æ˜¯è¯´å…¶ä¸­kè¦æ¯”nå°å¾ˆå¤šï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªå¤§çš„çŸ©é˜µAå¯ä»¥ç”¨ä¸‰ä¸ªå°çš„çŸ©é˜µğ‘ˆğ‘šÃ—ğ‘˜,Î£ğ‘˜Ã—ğ‘˜,ğ‘‰ğ‘‡ğ‘˜Ã—ğ‘›æ¥è¡¨ç¤ºã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç°åœ¨æˆ‘ä»¬çš„çŸ©é˜µAåªéœ€è¦ç°è‰²çš„éƒ¨åˆ†çš„ä¸‰ä¸ªå°çŸ©é˜µå°±å¯ä»¥è¿‘ä¼¼æè¿°äº†ã€‚
```

## äºŒã€chp3

### åºåˆ—æ ‡æ³¨

- crf

- ç»´ç‰¹æ¯”ç®—æ³•ï¼ˆViterbiï¼‰è§£ç ç®—æ³•

- è¯­æ³•æ ‘è§£æ

- ä¾å­˜å¥æ³•åˆ†æ

- hmm:ã€æ•°ä¹‹é“ 29ã€‘"éšé©¬å°”å¯å¤«æ¨¡å‹"HMMæ˜¯ä»€ä¹ˆï¼Ÿäº†è§£å®ƒåªéœ€5åˆ†é’Ÿï¼https://www.bilibili.com/video/BV1ZB4y1y7gC?spm_id_from=333.337.search-card.all.click&vd_source=953ee6e625bf100ef80ae35363880522



## å››ã€åŸºç¡€ä»£ç å­¦ä¹ 

### 4.1 mlp
```python
import torch
from torch import nn, optim
from torch.nn import functional as F

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_class):
        super(MLP, self).__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.activate = F.relu
        self.linear2 = nn.Linear(hidden_dim, num_class)

    def forward(self, inputs):
        hidden = self.linear1(inputs)
        activation = self.activate(hidden)
        outputs = self.linear2(activation)
        # è·å¾—æ¯ä¸ªè¾“å…¥å±äºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡ï¼ˆSoftmaxï¼‰ï¼Œç„¶åå†å–å¯¹æ•°
        # å–å¯¹æ•°çš„ç›®çš„æ˜¯é¿å…è®¡ç®—Softmaxæ—¶å¯èƒ½äº§ç”Ÿçš„æ•°å€¼æº¢å‡ºé—®é¢˜
        # log_softmax + NLLLoss() å°±æ˜¯äº¤å‰ç†µçš„ç»“æœ
        # Pytorchè¯¦è§£NLLLosså’ŒCrossEntropyLoss : https://blog.51cto.com/u_15274944/2921745
        # æŸå¤±å‡½æ•°ï½œäº¤å‰ç†µæŸå¤±å‡½æ•° ï¼šhttps://zhuanlan.zhihu.com/p/35709485
        log_probs = F.log_softmax(outputs, dim=1)
        return log_probs

# å¼‚æˆ–é—®é¢˜çš„4ä¸ªè¾“å…¥
x_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
# æ¯ä¸ªè¾“å…¥å¯¹åº”çš„è¾“å‡ºç±»åˆ«
y_train = torch.tensor([0, 1, 1, 0])

# åˆ›å»ºå¤šå±‚æ„ŸçŸ¥å™¨æ¨¡å‹ï¼Œè¾“å…¥å±‚å¤§å°ä¸º2ï¼Œéšå«å±‚å¤§å°ä¸º5ï¼Œè¾“å‡ºå±‚å¤§å°ä¸º2ï¼ˆå³æœ‰ä¸¤ä¸ªç±»åˆ«ï¼‰
model = MLP(input_dim=2, hidden_dim=5, num_class=2)

criterion = nn.NLLLoss() # å½“ä½¿ç”¨log_softmaxè¾“å‡ºæ—¶ï¼Œéœ€è¦è°ƒç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆNegative Log Likelihoodï¼ŒNLLï¼‰
optimizer = optim.SGD(model.parameters(), lr=0.05) # ä½¿ç”¨æ¢¯åº¦ä¸‹é™å‚æ•°ä¼˜åŒ–æ–¹æ³•ï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸º0.05

for epoch in range(100):
    y_pred = model(x_train) # è°ƒç”¨æ¨¡å‹ï¼Œé¢„æµ‹è¾“å‡ºç»“æœ
    loss = criterion(y_pred, y_train) # é€šè¿‡å¯¹æ¯”é¢„æµ‹ç»“æœä¸æ­£ç¡®çš„ç»“æœï¼Œè®¡ç®—æŸå¤±
    optimizer.zero_grad() # åœ¨è°ƒç”¨åå‘ä¼ æ’­ç®—æ³•ä¹‹å‰ï¼Œå°†ä¼˜åŒ–å™¨çš„æ¢¯åº¦å€¼ç½®ä¸ºé›¶ï¼Œå¦åˆ™æ¯æ¬¡å¾ªç¯çš„æ¢¯åº¦å°†è¿›è¡Œç´¯åŠ 
    loss.backward() # é€šè¿‡åå‘ä¼ æ’­è®¡ç®—å‚æ•°çš„æ¢¯åº¦
    optimizer.step() # åœ¨ä¼˜åŒ–å™¨ä¸­æ›´æ–°å‚æ•°ï¼Œä¸åŒä¼˜åŒ–å™¨æ›´æ–°çš„æ–¹æ³•ä¸åŒï¼Œä½†æ˜¯è°ƒç”¨æ–¹å¼ç›¸åŒ

print("Parameters:")
for name, param in model.named_parameters():
    print (name, param.data)

y_pred = model(x_train)
print("Predicted results:", y_pred.argmax(axis=1))

```

### 4.2 embedding
```python
import torch
from torch import nn
from torch.nn import functional as F

class MLP(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):
        super(MLP, self).__init__()
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # çº¿æ€§å˜æ¢ï¼šè¯åµŒå…¥å±‚->éšå«å±‚
        self.linear1 = nn.Linear(embedding_dim, hidden_dim)
        # ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
        self.activate = F.relu
        # çº¿æ€§å˜æ¢ï¼šæ¿€æ´»å±‚->è¾“å‡ºå±‚
        self.linear2 = nn.Linear(hidden_dim, num_class)

    def forward(self, inputs):
        # B*S = 2 * 4
        embeddings = self.embedding(inputs)
        # å°†åºåˆ—ä¸­å¤šä¸ªembeddingè¿›è¡Œèšåˆï¼ˆæ­¤å¤„æ˜¯æ±‚å¹³å‡å€¼ï¼‰
        embedding = embeddings.mean(dim=1)
        hidden = self.activate(self.linear1(embedding))
        outputs = self.linear2(hidden)
        # è·å¾—æ¯ä¸ªåºåˆ—å±äºæŸä¸€ç±»åˆ«æ¦‚ç‡çš„å¯¹æ•°å€¼
        probs = F.log_softmax(outputs, dim=1)
        return probs

mlp = MLP(vocab_size=8, embedding_dim=3, hidden_dim=5, num_class=2)
# è¾“å…¥ä¸ºä¸¤ä¸ªé•¿åº¦ä¸º4çš„æ•´æ•°åºåˆ—
inputs = torch.tensor([[0, 1, 2, 1], [4, 6, 6, 7]], dtype=torch.long)
outputs = mlp(inputs)
print(outputs)
```

### 4.t lstm

```python
# Defined in Section 4.7.2

import torch
from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
from collections import defaultdict
from vocab import Vocab
from utils import load_treebank

#tqdmæ˜¯ä¸€ä¸ªPythonæ¨¡å—ï¼Œèƒ½ä»¥è¿›åº¦æ¡çš„æ–¹å¼æ˜¾å¼è¿­ä»£çš„è¿›åº¦
from tqdm.auto import tqdm

WEIGHT_INIT_RANGE = 0.1

class LstmDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def collate_fn(examples):
    lengths = torch.tensor([len(ex[0]) for ex in examples])
    inputs = [torch.tensor(ex[0]) for ex in examples]
    targets = [torch.tensor(ex[1]) for ex in examples]
    inputs = pad_sequence(inputs, batch_first=True, padding_value=vocab["<pad>"])
    targets = pad_sequence(targets, batch_first=True, padding_value=vocab["<pad>"])
    return inputs, lengths, targets, inputs != vocab["<pad>"]


def init_weights(model):
    for param in model.parameters():
        torch.nn.init.uniform_(param, a=-WEIGHT_INIT_RANGE, b=WEIGHT_INIT_RANGE) # å‡åŒ€åˆ†å¸ƒ

class LSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):
        super(LSTM, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.output = nn.Linear(hidden_dim, num_class)
        init_weights(self)

    def forward(self, inputs, lengths):
        embeddings = self.embeddings(inputs)
        x_pack = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)
        hidden, (hn, cn) = self.lstm(x_pack)
        hidden, _ = pad_packed_sequence(hidden, batch_first=True)
        outputs = self.output(hidden)
        log_probs = F.log_softmax(outputs, dim=-1)
        return log_probs

embedding_dim = 128
hidden_dim = 256
batch_size = 32
num_epoch = 5

#åŠ è½½æ•°æ®
train_data, test_data, vocab, pos_vocab = load_treebank()
train_dataset = LstmDataset(train_data)
test_dataset = LstmDataset(test_data)
train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)
test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)

num_class = len(pos_vocab)

#åŠ è½½æ¨¡å‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = LSTM(len(vocab), embedding_dim, hidden_dim, num_class)
model.to(device) #å°†æ¨¡å‹åŠ è½½åˆ°GPUä¸­ï¼ˆå¦‚æœå·²ç»æ­£ç¡®å®‰è£…ï¼‰

#è®­ç»ƒè¿‡ç¨‹
nll_loss = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001) #ä½¿ç”¨Adamä¼˜åŒ–å™¨

model.train()
for epoch in range(num_epoch):
    total_loss = 0
    for batch in tqdm(train_data_loader, desc=f"Training Epoch {epoch}"):
        inputs, lengths, targets, mask = [x.to(device) for x in batch]
        log_probs = model(inputs, lengths)
        loss = nll_loss(log_probs[mask], targets[mask])
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Loss: {total_loss:.2f}")

#æµ‹è¯•è¿‡ç¨‹
acc = 0
total = 0
for batch in tqdm(test_data_loader, desc=f"Testing"):
    inputs, lengths, targets, mask = [x.to(device) for x in batch]
    with torch.no_grad():
        output = model(inputs, lengths)
        acc += (output.argmax(dim=-1) == targets)[mask].sum().item()
        total += mask.sum().item()

#è¾“å‡ºåœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡
print(f"Acc: {acc / total:.2f}")

```

## äº”ã€ç¬¬äº”ç« 

### 5.1 CBOW
```python

# Defined in Section 5.2.3.1

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence
from tqdm.auto import tqdm
from utils import BOS_TOKEN, EOS_TOKEN, PAD_TOKEN
from utils import load_reuters, save_pretrained, get_loader, init_weights

class CbowDataset(Dataset):
    def __init__(self, corpus, vocab, context_size=2):
        self.data = []
        self.bos = vocab[BOS_TOKEN]
        self.eos = vocab[EOS_TOKEN]
        for sentence in tqdm(corpus, desc="Dataset Construction"):
            sentence = [self.bos] + sentence+ [self.eos]
            if len(sentence) < context_size * 2 + 1:
                continue
            for i in range(context_size, len(sentence) - context_size):
                # æ¨¡å‹è¾“å…¥ï¼šå·¦å³åˆ†åˆ«å–context_sizeé•¿åº¦çš„ä¸Šä¸‹æ–‡
                context = sentence[i-context_size:i] + sentence[i+1:i+context_size+1]
                # æ¨¡å‹è¾“å‡ºï¼šå½“å‰è¯
                target = sentence[i]
                self.data.append((context, target))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, i):
        return self.data[i]

    def collate_fn(self, examples):
        inputs = torch.tensor([ex[0] for ex in examples])
        targets = torch.tensor([ex[1] for ex in examples])
        return (inputs, targets)

class CbowModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(CbowModel, self).__init__()
        # è¯åµŒå…¥å±‚
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        # çº¿æ€§å˜æ¢ï¼šéšå«å±‚->è¾“å‡ºå±‚
        self.output = nn.Linear(embedding_dim, vocab_size)
        init_weights(self)

    def forward(self, inputs):
        embeds = self.embeddings(inputs)
        # è®¡ç®—éšå«å±‚ï¼šå¯¹ä¸Šä¸‹æ–‡è¯å‘é‡æ±‚å¹³å‡
        hidden = embeds.mean(dim=1)
        output = self.output(hidden)
        log_probs = F.log_softmax(output, dim=1)
        return log_probs

embedding_dim = 64
context_size = 2
hidden_dim = 128
batch_size = 1024
num_epoch = 10

# è¯»å–æ–‡æœ¬æ•°æ®ï¼Œæ„å»ºCBOWæ¨¡å‹è®­ç»ƒæ•°æ®é›†
corpus, vocab = load_reuters()
dataset = CbowDataset(corpus, vocab, context_size=context_size)
data_loader = get_loader(dataset, batch_size)

nll_loss = nn.NLLLoss()
# æ„å»ºCBOWæ¨¡å‹ï¼Œå¹¶åŠ è½½è‡³device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CbowModel(len(vocab), embedding_dim)
model.to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model.train()
for epoch in range(num_epoch):
    total_loss = 0
    for batch in tqdm(data_loader, desc=f"Training Epoch {epoch}"):
        inputs, targets = [x.to(device) for x in batch]
        optimizer.zero_grad()
        log_probs = model(inputs)
        loss = nll_loss(log_probs, targets)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Loss: {total_loss:.2f}")

# ä¿å­˜è¯å‘é‡ï¼ˆmodel.embeddingsï¼‰
save_pretrained(vocab, model.embeddings.weight.data, "cbow.vec")


```

### 5.2 Glove

```python
# Defined in Section 5.3.4

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence
from tqdm.auto import tqdm
from utils import BOS_TOKEN, EOS_TOKEN, PAD_TOKEN
from utils import load_reuters, save_pretrained, get_loader, init_weights
from collections import defaultdict

class GloveDataset(Dataset):
    def __init__(self, corpus, vocab, context_size=2):
        # è®°å½•è¯ä¸ä¸Šä¸‹æ–‡åœ¨ç»™å®šè¯­æ–™ä¸­çš„å…±ç°æ¬¡æ•°
        self.cooccur_counts = defaultdict(float)
        self.bos = vocab[BOS_TOKEN]
        self.eos = vocab[EOS_TOKEN]
        for sentence in tqdm(corpus, desc="Dataset Construction"):
            sentence = [self.bos] + sentence + [self.eos]
            for i in range(1, len(sentence)-1):
                w = sentence[i]
                left_contexts = sentence[max(0, i - context_size):i]
                right_contexts = sentence[i+1:min(len(sentence), i + context_size)+1]
                # å…±ç°æ¬¡æ•°éšè·ç¦»è¡°å‡: 1/d(w, c)
                for k, c in enumerate(left_contexts[::-1]):
                    self.cooccur_counts[(w, c)] += 1 / (k + 1)
                for k, c in enumerate(right_contexts):
                    self.cooccur_counts[(w, c)] += 1 / (k + 1)
        self.data = [(w, c, count) for (w, c), count in self.cooccur_counts.items()]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, i):
        return self.data[i]

    def collate_fn(self, examples):
        words = torch.tensor([ex[0] for ex in examples])
        contexts = torch.tensor([ex[1] for ex in examples])
        counts = torch.tensor([ex[2] for ex in examples])
        return (words, contexts, counts)

class GloveModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(GloveModel, self).__init__()
        # è¯åµŒå…¥åŠåç½®å‘é‡
        self.w_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.w_biases = nn.Embedding(vocab_size, 1)
        # ä¸Šä¸‹æ–‡åµŒå…¥åŠåç½®å‘é‡
        self.c_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.c_biases = nn.Embedding(vocab_size, 1)

    def forward_w(self, words):
        w_embeds = self.w_embeddings(words)
        w_biases = self.w_biases(words)
        return w_embeds, w_biases

    def forward_c(self, contexts):
        c_embeds = self.c_embeddings(contexts)
        c_biases = self.c_biases(contexts)
        return c_embeds, c_biases

embedding_dim = 64
context_size = 2
batch_size = 1024
num_epoch = 10

# ç”¨ä»¥æ§åˆ¶æ ·æœ¬æƒé‡çš„è¶…å‚æ•°
m_max = 100
alpha = 0.75
# ä»æ–‡æœ¬æ•°æ®ä¸­æ„å»ºGloVeè®­ç»ƒæ•°æ®é›†
corpus, vocab = load_reuters()
dataset = GloveDataset(
    corpus,
    vocab,
    context_size=context_size
)
data_loader = get_loader(dataset, batch_size)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GloveModel(len(vocab), embedding_dim)
model.to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model.train()
for epoch in range(num_epoch):
    total_loss = 0
    for batch in tqdm(data_loader, desc=f"Training Epoch {epoch}"):
        words, contexts, counts = [x.to(device) for x in batch]
        # æå–batchå†…è¯ã€ä¸Šä¸‹æ–‡çš„å‘é‡è¡¨ç¤ºåŠåç½®
        word_embeds, word_biases = model.forward_w(words)
        context_embeds, context_biases = model.forward_c(contexts)
        # å›å½’ç›®æ ‡å€¼ï¼šå¿…è¦æ—¶å¯ä»¥ä½¿ç”¨log(counts+1)è¿›è¡Œå¹³æ»‘
        log_counts = torch.log(counts)
        # æ ·æœ¬æƒé‡
        weight_factor = torch.clamp(torch.pow(counts / m_max, alpha), max=1.0) # clampï¼šå¤¹ç´§åˆ°ï¼ˆminï¼Œmaxï¼‰ä¹‹é—´
        optimizer.zero_grad()
        # è®¡ç®—batchå†…æ¯ä¸ªæ ·æœ¬çš„L2æŸå¤±
        loss = (torch.sum(word_embeds * context_embeds, dim=1) + word_biases + context_biases - log_counts) ** 2
        # æ ·æœ¬åŠ æƒæŸå¤±
        wavg_loss = (weight_factor * loss).mean()
        wavg_loss.backward()
        optimizer.step()
        total_loss += wavg_loss.item()
    print(f"Loss: {total_loss:.2f}")

# åˆå¹¶è¯åµŒå…¥çŸ©é˜µä¸ä¸Šä¸‹æ–‡åµŒå…¥çŸ©é˜µï¼Œä½œä¸ºæœ€ç»ˆçš„é¢„è®­ç»ƒè¯å‘é‡
combined_embeds = model.w_embeddings.weight + model.c_embeddings.weight
save_pretrained(vocab, combined_embeds.data, "glove.vec")


```

## ä¸ƒã€ç¬¬ä¸ƒç« 

### 7.1 sståˆ†ç±»

```python
# Defined in Section 7.4.2.2

import numpy as np
from datasets import load_dataset, load_metric
from transformers import BertTokenizerFast, BertForSequenceClassification, TrainingArguments, Trainer

# åŠ è½½è®­ç»ƒæ•°æ®ã€åˆ†è¯å™¨ã€é¢„è®­ç»ƒæ¨¡å‹ä»¥åŠè¯„ä»·æ–¹æ³•
dataset = load_dataset('glue', 'sst2')
tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')
model = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict=True)
metric = load_metric('glue', 'sst2')

# å¯¹è®­ç»ƒé›†è¿›è¡Œåˆ†è¯
def tokenize(examples):
    return tokenizer(examples['sentence'], truncation=True, padding='max_length')
dataset = dataset.map(tokenize, batched=True)
encoded_dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)

# å°†æ•°æ®é›†æ ¼å¼åŒ–ä¸ºtorch.Tensorç±»å‹ä»¥è®­ç»ƒPyTorchæ¨¡å‹
columns = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']
encoded_dataset.set_format(type='torch', columns=columns)

# å®šä¹‰è¯„ä»·æŒ‡æ ‡
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=np.argmax(predictions, axis=1), references=labels)

# å®šä¹‰è®­ç»ƒå‚æ•°TrainingArgumentsï¼Œé»˜è®¤ä½¿ç”¨AdamWä¼˜åŒ–å™¨
args = TrainingArguments(
    "ft-sst2",                          # è¾“å‡ºè·¯å¾„ï¼Œå­˜æ”¾æ£€æŸ¥ç‚¹å’Œå…¶ä»–è¾“å‡ºæ–‡ä»¶
    evaluation_strategy="epoch",        # å®šä¹‰æ¯è½®ç»“æŸåè¿›è¡Œè¯„ä»·
    learning_rate=2e-5,                 # å®šä¹‰åˆå§‹å­¦ä¹ ç‡
    per_device_train_batch_size=16,     # å®šä¹‰è®­ç»ƒæ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size=16,      # å®šä¹‰æµ‹è¯•æ‰¹æ¬¡å¤§å°
    num_train_epochs=2,                 # å®šä¹‰è®­ç»ƒè½®æ•°
)

# å®šä¹‰Trainerï¼ŒæŒ‡å®šæ¨¡å‹å’Œè®­ç»ƒå‚æ•°ï¼Œè¾“å…¥è®­ç»ƒé›†ã€éªŒè¯é›†ã€åˆ†è¯å™¨ä»¥åŠè¯„ä»·å‡½æ•°
trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# å¼€å§‹è®­ç»ƒï¼ï¼ˆä¸»æµGPUä¸Šè€—æ—¶çº¦å‡ å°æ—¶ï¼‰
trainer.train()

```

### 7.2 tokenizerå¤„ç†é•¿æ–‡æœ¬æ—¶æˆªæ–­
https://xiaosheng.run/2022/03/08/transformers-note-5.html#%E5%A4%84%E7%90%86%E9%95%BF%E6%96%87%E6%9C%AC
- return_overflowing_tokens=Trueã€‚è€ƒè™‘åˆ°å¦‚æœæˆªæ–­çš„ä½ç½®ä¸åˆç†ï¼Œä¹Ÿå¯èƒ½æ— æ³•æŠ½å–å‡ºæ­£ç¡®çš„ç­”æ¡ˆï¼Œå› æ­¤è¿˜å¯ä»¥é€šè¿‡è®¾ç½®æ­¥é•¿å‚æ•° stride æ§åˆ¶æ–‡æœ¬å—é‡å éƒ¨åˆ†çš„é•¿åº¦ã€‚ä¾‹å¦‚ï¼š
```python
# stride=2ä¸ºæˆªæ–­çš„æ—¶å€™ï¼Œé‡å¤çš„é•¿åº¦
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```
- output
```
[CLS] This sentence is not [SEP]
[CLS] is not too long [SEP]
[CLS] too long but we [SEP]
[CLS] but we are going [SEP]
[CLS] are going to split [SEP]
[CLS] to split it anyway [SEP]
[CLS] it anyway. [SEP]
```

### 7.3 offset: æ ‡è®°åŸæ–‡æ¯ä¸ªspançš„èµ·å§‹ä½ç½®
https://huggingface.co/course/chapter6/3
```
This is very similar to what we had before, with one exception: the pipeline also gave us information about the start and end of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set return_offsets_mapping=True when we apply the tokenizer to our inputs:

Copied
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
Copied
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
Each tuple is the span of text corresponding to each token, where (0, 0) is reserved for the special tokens. We saw before that the token at index 5 is ##yl, which has (12, 14) as offsets here. If we grab the corresponding slice in our example:

Copied
example[12:14]
we get the proper span of text without the ##:

Copied
yl
Using this, we can now complete the previous results:

Copied
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
Copied
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
This is the same as what we got from the first pipeline!
```